batch_size: 1024
start_steps: 3000
n_steps: 1000001
num_cluster: 40  # ! Num_clusters
save_name: '0.01_exp4' #! Name of the model
seed: 42
replay_size: 1000000 
update_per_step: 1
use_per: True # ! Whether to use Prioritized Experience Replay

# PER-specific hyperparameters
per_alpha: 0.6
per_beta: 0.4

# Evaluation interval (in number of updates)
eval_interval: 10000  # 1만 업데이트마다 평가 (훨씬 덜 자주)
eval_episodes: 20     # 20개 에피소드로 빠르게

env_args:
  returns_data: './data/log_returns_by_month.pkl'
  use_winsorize: False
  clusters_dir: './res/batch_1024_n_bins_64_hidden_128_std_0.1_mask_0.1_ctau_1.0/predictions/'
  start_month: '1995-01'
  end_month: '2005-12'
  reward_scale: 1 #* Scale 
  num_inputs: 33
  hard_reward: 'Sharpe' # ! Use new DirectSharpe reward function
  dynamic_gamma: 0.01 # ! Set to 0 to provide a clean reward signal

agent_args:
  agent_type: "GRPO"    # Type of agent
  num_inputs: 33 #25-03-11 : State 증가 
  num_action: 2
  action_space: [[0.8, 0.3] , [2.0, 0.7]] # 첫번째 차원(thres) 최소값 0.8, 최대값 2.0 / 두번째 차원(outlier) 최소값 0.3, 최대값 0.7
  gamma: 0.99
  tau: 0.005
  lr: 0.0003  # 학습률을 1/10로 줄여서 안정화
  kl_weight : 0 
  alpha: 0.2
  group_size: 8 # ! group size
  target_update_interval: 1
  hidden_size: [256,256,256]

