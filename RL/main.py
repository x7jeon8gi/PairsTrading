# ê²½ë¡œ í˜¸í™˜ì„± í™•ë³´í•  ê²ƒ

from env import TradingEnvironment
from sac import SAC
from replay_memory import ReplayMemory
import torch
import argparse
import datetime
import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt
import yaml
import os
from tqdm import tqdm
import wandb
import sys
import json
import time
from grpo import GRPOAgent
from ppo import PPOAgent
from utils.seed import seed_everything
from utils import calculate_metrics
from utils.metrics import calculate_financial_metrics_monthly
import logging
from collector import DataCollector
import torch.multiprocessing as mp

# Set the start method for multiprocessing
# This must be done at the very beginning, right after imports
# and before any other multiprocessing-related code is executed.
if __name__ == "__main__":
    try:
        mp.set_start_method('spawn')
    except RuntimeError:
        pass


def override_config(config, args):
    args_dict = vars(args)
    for key, value in args_dict.items():
        if value is not None:
            if key.startswith('env_args_'):
                # env_args ê´€ë ¨ ì¸ì
                sub_key = key.replace('env_args_', '')
                config['env_args'][sub_key] = value
            elif key.startswith('agent_args_'):
                # agent_args ê´€ë ¨ ì¸ì
                sub_key = key.replace('agent_args_', '')
                if sub_key == 'action_space':
                    # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    config['agent_args'][sub_key] = [float(x) for x in value.split(',')]
                else:
                    config['agent_args'][sub_key] = value
            else:
                # ìµœìƒìœ„ ì¸ì
                config[key] = value
    return config

def set_logger(seed, run_name, save_dir):
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(formatter)
    if not os.path.exists(f'{save_dir}/log'):
        os.makedirs(f'{save_dir}/log')
    fh = logging.FileHandler(filename=f'{save_dir}/log/logging_{seed}_{run_name}.log')
    fh.setLevel(logging.DEBUG)
    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger

class Trainer(object):
    def __init__(self,
                 batch_size,
                 start_steps, # Number of steps for which the model picks random actions
                 n_steps,     # Maximum number of steps
                 save_name,
                 device,
                 seed,
                 env_args,   # returns_data, clusters_dir, start_month, end_month, reward_scale, outlier_filter
                 agent_args, # num_inputs, action_space, gamma, tau, alpha, target_update_interval, hidden_size, lr
                 replay_size,
                 update_per_step,
                 logger,
                 use_per=False,
                 per_alpha=0.6,
                 per_beta=0.4,
                 eval_interval=10,
                 eval_episodes=10,
                 num_collectors=2):

        #wandb.init(project='SAC_PairsTrading', name=save_name, config=config)

        self.batch_size = batch_size
        self.start_steps = start_steps
        self.n_steps = n_steps 
        self.save_name = save_name
        self.device = device
        self.seed = seed
        self.update_per_step = update_per_step
        self.env_args = env_args
        self.agent_args = agent_args
        self.num_actions = agent_args['num_action']
        self.eval_interval = eval_interval
        self.eval_episodes = eval_episodes
        self.use_per = use_per
        self.per_alpha = per_alpha
        self.per_beta = per_beta
        self.logger = logger
        self.num_collectors = num_collectors

        # ! í™˜ê²½ ì´ˆê¸°í™” -> Collectorë¡œ ì´ë™
        # self.env    = TradingEnvironment(**env_args)

        # ! ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (Learner)
        agent_type = agent_args.get("agent_type", "SAC")
        if agent_type == "SAC":
            from sac import SAC
            self.agent = SAC(**agent_args)
        elif agent_type == "PPO":
            from ppo import PPOAgent
            self.agent = PPOAgent(**agent_args)
        elif agent_type == "GRPO":
            # GRPOAgent expects state_dim, action_dim, hidden_sizes, etc.
            state_dim = agent_args['num_inputs']
            action_dim = agent_args['num_action']
            hidden_sizes = agent_args['hidden_size']  # should be a list, e.g., [64,64,64]
            action_space = agent_args['action_space']
            # ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ë„ agent_argsì—ì„œ ê°€ì ¸ì˜´
            lr = agent_args.get("lr", 0.0003)
            clip_epsilon = agent_args.get("clip_epsilon", 0.2)
            group_size = agent_args.get("group_size", 5)
            kl_weight = agent_args.get("kl_weight", 0.1)
            self.agent = GRPOAgent(state_dim, 
                                   action_dim, 
                                   hidden_sizes, 
                                   action_space, 
                                   lr=lr, 
                                   clip_epsilon=clip_epsilon, 
                                   group_size=group_size,
                                   kl_weight = kl_weight)
        else:
            raise ValueError("Unknown agent_type specified in agent_args")

        if self.use_per:
            from replay_memory import PrioritizedReplayMemory
            self.memory = PrioritizedReplayMemory(replay_size, seed, alpha=per_alpha, beta=per_beta)
            self.logger.info(f"ğŸ”¥ Using Prioritized Experience Replay (alpha={per_alpha}, beta={per_beta})")
            print(f"ğŸ”¥ PER ENABLED: Memory type = {type(self.memory).__name__}")
        else:
            from replay_memory import ReplayMemory
            self.memory = ReplayMemory(replay_size, seed)
            self.logger.info("ğŸ“ Using standard Replay Memory")
            print(f"ğŸ“ STANDARD MEMORY: Memory type = {type(self.memory).__name__}")
            
        self.max_avg_reward = -float('inf')

        # Create model directory if it doesn't exist
        os.makedirs('./res/RL/models', exist_ok=True)
        os.makedirs('./res/RL/rewards', exist_ok=True)
        
        # --- ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ ì„¤ì • ---
        self.experience_queue = mp.Queue(maxsize=1000)
        self.policy_queue = mp.Queue(maxsize=self.num_collectors)

        self.collectors = []
        for i in range(self.num_collectors):
            collector_seed = self.seed + i
            collector = DataCollector(
                self.env_args,
                self.agent_args,
                self.experience_queue,
                self.policy_queue,
                collector_seed,
                collector_id=i
            )
            collector.start()
            self.collectors.append(collector)
            self.logger.info(f"DataCollector {i} started with seed {collector_seed}.")


    def train_offline(self):
        """Offline training loop for SAC (replay memory based) using async data collection."""
        total_numsteps = 0
        updates = 0
        rewards = []
        progress_bar = tqdm(total=self.n_steps, desc="Training Progress")

        # ì´ˆê¸° ì •ì±…ì„ Collectorë“¤ì—ê²Œ í•œ ë²ˆ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        policy_state_dict = {k: v.cpu() for k, v in self.agent.actor.state_dict().items()}
        for _ in range(self.num_collectors):
            self.policy_queue.put(policy_state_dict)

        while updates < self.n_steps:
            # 1. ê²½í—˜ ë°ì´í„° ìˆ˜ì§‘ ë° ë¦¬í”Œë ˆì´ ë²„í¼ ì±„ìš°ê¸°
            try:
                state, action, reward, next_state, done = self.experience_queue.get(timeout=1)
                self.memory.push(state, action, reward, next_state, done)
                total_numsteps += 1
                progress_bar.update(1)
            except Exception as e:
                # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ê³„ì† í•™ìŠµ ì§„í–‰
                pass

            # 2. ë¦¬í”Œë ˆì´ ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¨ë©´ í•™ìŠµ ì‹œì‘
            if len(self.memory) > self.batch_size:
                for _ in range(self.update_per_step):
                    critic_1_loss, critic_2_loss, policy_loss, alpha_loss = self.agent.update_parameters(self.memory, self.batch_size, updates)
                    updates += 1

                    # ì£¼ê¸°ì ìœ¼ë¡œ Collectorë“¤ì—ê²Œ ìµœì‹  ì •ì±… ì „ì†¡
                    if updates % 200 == 0: 
                        policy_state_dict = {k: v.cpu() for k, v in self.agent.actor.state_dict().items()}
                        # íê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ ë„£ì–´ì„œ ë„ˆë¬´ ë§ì´ ìŒ“ì´ì§€ ì•Šë„ë¡ í•¨
                        if self.policy_queue.empty():
                            for _ in range(self.num_collectors):
                                self.policy_queue.put(policy_state_dict)

                    if updates % 1000 == 0:  # 1000ë²ˆë§ˆë‹¤ ì¶œë ¥ìœ¼ë¡œ ë³€ê²½
                        log_msg = (f"Updates: {updates} | C1 Loss: {critic_1_loss:.4f} | "
                                   f"C2 Loss: {critic_2_loss:.4f} | Policy Loss: {policy_loss:.4f} | "
                                   f"Alpha Loss: {alpha_loss:.4f} | Alpha: {self.agent.alpha.item():.4f}")
                        self.logger.info(log_msg)
                        print(f"ğŸ“ˆ Training Progress - Updates: {updates} | C1: {critic_1_loss:.2f} | C2: {critic_2_loss:.2f} | Policy: {policy_loss:.2f}")

            # 3. ì£¼ê¸°ì ì¸ í‰ê°€
            if updates > 0 and updates % self.eval_interval == 0: # ìˆ˜ì •: 1000 ê³±í•˜ê¸° ì œê±°
                avg_reward = self._evaluate_agent(self.eval_episodes)
                rewards.append(avg_reward)
                self.logger.info(f'Evaluation at Updates: {updates} | Total Steps: {total_numsteps} | Average Reward: {avg_reward:.4f}')

                if avg_reward > self.max_avg_reward:
                    self.max_avg_reward = avg_reward
                    self.agent.save_model('./res/RL/models', self.save_name)
                    self.logger.info(f'********** New best model saved with avg reward: {self.max_avg_reward:.4f} **********')

            if updates >= self.n_steps:
                break

        progress_bar.close()
        self._save_reward_plot(rewards, xlabel=f'Evaluation (every {self.eval_interval} updates)')
        self.logger.info("Offline training completed.")

        # í•™ìŠµ ì¢…ë£Œ í›„ Collector í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
        for collector in self.collectors:
            collector.terminate()
            collector.join()
        self.logger.info("All DataCollectors terminated.")


    def train_online(self):
        """Online training loop for PPO/GRPO (group-based updates)"""
        total_numsteps = 0
        rewards = []
        group_trajectories = []
        episode_num = 0
        group_size = self.agent_args.get("group_size", 5)
        progress_bar = tqdm(total=self.n_steps, desc="Training Progress")
        episode_rewards_history = [] # List to store episode rewards

        while total_numsteps < self.n_steps:
            # collect trajectories from old policy
            trajectory = self.agent.collect_trajectory(self.env, max_steps=200) # states, actions, log_porbs, rewards
            group_trajectories.append(trajectory)

            episode_reward = sum(trajectory['rewards'])
            episode_rewards_history.append(episode_reward) # Store episode reward
            total_numsteps += len(trajectory['states'])
            episode_num += 1

            if len(group_trajectories) >= group_size:
                update_loss = self.agent.grpo_update(group_trajectories, n_iterations=20) # n_iterations: êµ­ë°¥
                self.logger.info(f'Online Update at Episode: {episode_num} | Loss: {update_loss:.4f}')
                group_trajectories = []  # reset group

            self.logger.info(f'Episode: {episode_num} | Total Steps: {total_numsteps} | Episode Reward: {episode_reward:.4f}')

            if episode_num % self.eval_interval == 0:
                avg_reward = self._evaluate_agent(self.eval_episodes)
                rewards.append(avg_reward)
                self.logger.info(f'Evaluation at Episode: {episode_num} | Total Steps: {total_numsteps} | Average Reward: {avg_reward:.4f}')

                if avg_reward > self.max_avg_reward:
                    self.max_avg_reward = avg_reward
                    self.agent.save_model('./res/RL/models', self.save_name)
                    self.logger.info(f'********** New best model saved with avg reward: {self.max_avg_reward:.4f} **********')

            progress_bar.update(len(trajectory['states']))
            
            # Break loop if n_steps reached
            if total_numsteps >= self.n_steps:
                break

        progress_bar.close()
        if group_trajectories: # Process remaining trajectories if any
            update_loss = self.agent.grpo_update(group_trajectories, n_iterations=20)
            self.logger.info(f'Final Online Update | Loss: {update_loss:.4f}')

        # Save the plot of episode rewards
        self._save_reward_plot(episode_rewards_history, xlabel='Episode')
        self.logger.info("Online training completed.")

    def train(self):
        """Main train method selecting offline or online training based on agent type."""
        agent_type = self.agent_args.get("agent_type", "SAC")
        if agent_type == "SAC":
            self.train_offline()
        else:  # For PPO and GRPO
            self.train_online()

    @torch.no_grad()
    def _evaluate_agent(self, episodes):
        """Evaluate the agent's performance without affecting training - optimized version"""

        # í‰ê°€ìš© í™˜ê²½ì´ ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if not hasattr(self, 'eval_env'):
            self.eval_env = TradingEnvironment(**self.env_args)
        
        episode_rewards = []
        
        # Use evaluation mode for the agent (no exploration)
        for ep in range(episodes):
            state = self.eval_env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                action = self.agent.select_action(state, evaluate=True)
                next_state, reward, done  = self.eval_env.step(action)
                episode_reward += reward
                state = next_state
            
            episode_rewards.append(episode_reward)
            
            # Early stopping: ì¶©ë¶„í•œ ì—í”¼ì†Œë“œ(ìµœì†Œ 10ê°œ)ë¥¼ ì‹¤í–‰í•œ í›„ 
            # ë¶„ì‚°ì´ ì‘ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œí•˜ì—¬ í‰ê°€ ì‹œê°„ ë‹¨ì¶•
            if ep >= 9 and ep % 5 == 4:  # 10, 15, 20, ... ì—í”¼ì†Œë“œë§ˆë‹¤ ì²´í¬
                rewards_array = np.array(episode_rewards)
                std_error = np.std(rewards_array) / np.sqrt(len(rewards_array))
                if std_error < 0.1:  # í‘œì¤€ ì˜¤ì°¨ê°€ 0.1 ë¯¸ë§Œì´ë©´ ì¶©ë¶„íˆ ì•ˆì •ì 
                    break
    
        # Compute average reward
        avg_reward = np.mean(episode_rewards)
        return avg_reward

    def _save_reward_plot(self, rewards, xlabel=None):
        """Save a plot of the rewards"""
        import matplotlib.pyplot as plt

        plt.figure(figsize=(8, 6))
        plt.plot(rewards)
        # Use provided xlabel if available, otherwise use default based on eval_interval
        if xlabel is None:
            xlabel = 'Evaluation (every {} episodes)'.format(self.eval_interval)
        plt.xlabel(xlabel)
        plt.ylabel('Average Reward' if xlabel.startswith('Evaluation') else 'Episode Reward') # Adjust ylabel based on xlabel
        plt.title('Training Progress')
        plt.savefig(f'./res/RL/rewards/{self.save_name}.png')
        plt.close()

    def inference(self, model_dir, model_name):
        """
        ê°•í™”í•™ìŠµ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì „ì²´ ê¸°ê°„ì— ëŒ€í•´ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            model_dir: ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            model_name: ëª¨ë¸ íŒŒì¼ ì´ë¦„
            
        Returns:
            tuple: ì´ ë³´ìƒ, ìŠ¤í…ë³„ ë³´ìƒ, ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸, í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë¦¬ìŠ¤íŠ¸, ìƒíƒœ ì •ë³´
        """
        # ëª¨ë¸ ë¡œë“œ
        try:
            self.agent.load_model(model_dir, model_name)
            self.logger.info(f"ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {model_dir}/{model_name}")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e
            
        # í™˜ê²½ ì„¤ì • ì—…ë°ì´íŠ¸
        self.env_args['start_month'] = '2006-01'
        self.env_args['end_month'] = '2023-12'
        self.env_args['new_data'] = True
        self.env = TradingEnvironment(**self.env_args, sp500_data='./data/sp500.csv')
        
        self.logger.info(f"ì¶”ë¡  ì‹œì‘: ê¸°ê°„ {self.env_args['start_month']}~{self.env_args['end_month']}")

        # í™˜ê²½ ì´ˆê¸°í™”
        state = self.env.reset()
        done = False
        total_reward = 0
        
        # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        actions = []
        rewards = []
        portfolio_values = []
        states = []
        action_info = []  # ì•¡ì…˜ì˜ ìƒì„¸ ì •ë³´ ì €ì¥
        month_dates = []  # ì›”ë³„ ë‚ ì§œ ì •ë³´
        
        # ì¶”ë¡  ì‹¤í–‰
        step_count = 0
        while not done:
            # í˜„ì¬ ë‚ ì§œ ì €ì¥
            if hasattr(self.env, 'current_month'):
                month_dates.append(self.env.current_month)
            
            # ì•¡ì…˜ ì„ íƒ ë° ì‹¤í–‰
            action = self.agent.select_action(state, evaluate=True)
            next_state, reward, done = self.env.step(action)
            
            # ê²°ê³¼ ì €ì¥
            actions.append(action)
            rewards.append(reward)
            portfolio_values.append(self.env.current_portfolio_value)
            states.append(state)
            
            # ì•¡ì…˜ ìƒì„¸ ì •ë³´ ê¸°ë¡
            action_detail = {
                'step': step_count,
                'threshold': float(action[0]),
                'outlier_filter': float(action[1]),
                'reward': float(reward),
                'portfolio_value': float(self.env.current_portfolio_value)
            }
            
            if hasattr(self.env, 'current_month'):
                action_detail['month'] = self.env.current_month
                
            action_info.append(action_detail)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            total_reward += reward
            state = next_state
            step_count += 1
            
            # ë¡œê¹…
            if step_count % 10 == 0:
                self.logger.info(f"ìŠ¤í… {step_count}: ë³´ìƒ {reward:.4f}, í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: {self.env.current_portfolio_value:.4f}")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        self.logger.info(f"ì¶”ë¡  ì™„ë£Œ: ì´ {step_count}ê°œ ìŠ¤í…, ì´ ë³´ìƒ: {total_reward:.4f}, ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: {self.env.current_portfolio_value:.4f}")
        
        # ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„
        # self._visualize_results(portfolio_values, rewards, actions, month_dates)
        
        # ì•¡ì…˜ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        action_df = pd.DataFrame(action_info)
        if 'month' in action_df.columns:
            action_df.set_index('month', inplace=True)
        
        os.makedirs('./res/RL/results', exist_ok=True)
        action_df.to_csv(f'./res/RL/results/action_details_{model_name}.csv')
        self.logger.info(f"ì•¡ì…˜ ìƒì„¸ ì •ë³´ ì €ì¥ ì™„ë£Œ: ./res/RL/results/action_details_{model_name}.csv")
        
        # Convert RL agent's portfolio values (cumulative log returns) into a series
        cum_log_returns_rl = pd.Series(portfolio_values)
        
        # ë‚ ì§œ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì´ë¥¼ í™œìš©
        if month_dates:
            cum_log_returns_rl.index = month_dates
            
        # Compute the per-step log returns for RL agent
        log_returns_rl = cum_log_returns_rl.diff().fillna(0)  # Use 0 for the first step's diff
        
        # ìµœì¢… ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
        cumulative_return_rl = np.expm1(portfolio_values[-1]) if portfolio_values else 0
        self.logger.info(f"RL ëª¨ë¸ ìµœì¢… ëˆ„ì  ìˆ˜ìµë¥ : {cumulative_return_rl:.4f} ({cumulative_return_rl*100:.2f}%)")
        
        # ë°˜í™˜ ê°’ ì¶”ê°€: statesë¥¼ í¬í•¨í•˜ì—¬ ë” ìì„¸í•œ ë¶„ì„ ê°€ëŠ¥í•˜ë„ë¡ í•¨
        return total_reward, rewards, actions, portfolio_values, states
        
    def _visualize_results(self, portfolio_values, rewards, actions, dates=None):
        """
        RL ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
        
        Args:
            portfolio_values: í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë¦¬ìŠ¤íŠ¸
            rewards: ë³´ìƒ ë¦¬ìŠ¤íŠ¸
            actions: ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸
            dates: ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ ì‚¬í•­)
        """
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates
        import numpy as np
        from datetime import datetime
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('./res/RL/plots', exist_ok=True)
        
        # 1. í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ (ëˆ„ì  ìˆ˜ìµë¥ ) ê·¸ë˜í”„
        plt.figure(figsize=(12, 6))
        
        if dates and len(dates) == len(portfolio_values):
            # ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            date_objects = [datetime.strptime(date, '%Y-%m') for date in dates]
            plt.plot(date_objects, portfolio_values, 'b-', linewidth=2)
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            plt.gca().xaxis.set_major_locator(mdates.YearLocator())
        else:
            plt.plot(portfolio_values, 'b-', linewidth=2)
        
        plt.title('RL ëª¨ë¸ì˜ í¬íŠ¸í´ë¦¬ì˜¤ ëˆ„ì  ë¡œê·¸ ìˆ˜ìµë¥ ')
        plt.xlabel('ì›”')
        plt.ylabel('ëˆ„ì  ë¡œê·¸ ìˆ˜ìµë¥ ')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(f'./res/RL/plots/{self.save_name}_portfolio_value.png')
        plt.close()
        
        # 2. ì›”ë³„ ìˆ˜ìµë¥  ê·¸ë˜í”„ (ë¡œê·¸ ìˆ˜ìµë¥ ì˜ ì°¨ë¶„)
        monthly_returns = np.diff(portfolio_values, prepend=0)
        
        plt.figure(figsize=(12, 6))
        
        if dates and len(dates) == len(monthly_returns):
            date_objects = [datetime.strptime(date, '%Y-%m') for date in dates]
            # í”ŒëŸ¬ìŠ¤/ë§ˆì´ë„ˆìŠ¤ì— ë”°ë¼ ë§‰ëŒ€ ìƒ‰ìƒ ë³€ê²½
            colors = ['red' if x < 0 else 'green' for x in monthly_returns]
            plt.bar(date_objects, monthly_returns, color=colors, width=20)
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            plt.gca().xaxis.set_major_locator(mdates.YearLocator())
        else:
            colors = ['red' if x < 0 else 'green' for x in monthly_returns]
            plt.bar(range(len(monthly_returns)), monthly_returns, color=colors)
        
        plt.title('RL ëª¨ë¸ì˜ ì›”ë³„ ë¡œê·¸ ìˆ˜ìµë¥ ')
        plt.xlabel('ì›”')
        plt.ylabel('ì›”ë³„ ë¡œê·¸ ìˆ˜ìµë¥ ')
        plt.grid(True, axis='y')
        plt.tight_layout()
        plt.savefig(f'./res/RL/plots/{self.save_name}_monthly_returns.png')
        plt.close()
        
        # 3. ì•¡ì…˜ ë¶„í¬ ë¶„ì„
        if actions and len(actions) > 0:
            # ì•¡ì…˜ ì¶”ì¶œ
            actions_array = np.array(actions)
            thresholds = actions_array[:, 0]
            outlier_filters = actions_array[:, 1]
            
            # 3-1. ì„ê³„ê°’(threshold) ë¶„í¬
            plt.figure(figsize=(10, 5))
            plt.plot(thresholds, 'r-', label='Threshold')
            plt.title('RL ëª¨ë¸ì˜ ì„ê³„ê°’(threshold) ë³€í™”')
            plt.xlabel('ìŠ¤í…')
            plt.ylabel('ì„ê³„ê°’')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig(f'./res/RL/plots/{self.save_name}_thresholds.png')
            plt.close()
            
            # 3-2. ì•„ì›ƒë¼ì´ì–´ í•„í„° ë¶„í¬
            plt.figure(figsize=(10, 5))
            plt.plot(outlier_filters, 'g-', label='Outlier Filter')
            plt.title('RL ëª¨ë¸ì˜ ì•„ì›ƒë¼ì´ì–´ í•„í„° ë³€í™”')
            plt.xlabel('ìŠ¤í…')
            plt.ylabel('ì•„ì›ƒë¼ì´ì–´ í•„í„° ê°’')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig(f'./res/RL/plots/{self.save_name}_outlier_filters.png')
            plt.close()
            
            # 3-3. ì„ê³„ê°’ê³¼ ì•„ì›ƒë¼ì´ì–´ í•„í„°ì˜ ê´€ê³„ (ì‚°ì ë„)
            plt.figure(figsize=(8, 8))
            plt.scatter(thresholds, outlier_filters, alpha=0.6, c=range(len(thresholds)), cmap='viridis')
            plt.colorbar(label='Step')
            plt.title('RL ëª¨ë¸ì˜ ì„ê³„ê°’ vs ì•„ì›ƒë¼ì´ì–´ í•„í„°')
            plt.xlabel('ì„ê³„ê°’(threshold)')
            plt.ylabel('ì•„ì›ƒë¼ì´ì–´ í•„í„°')
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(f'./res/RL/plots/{self.save_name}_action_scatter.png')
            plt.close()
        
        # 4. ì›”ë³„ ë³´ìƒ ê·¸ë˜í”„
        plt.figure(figsize=(12, 6))
        
        if dates and len(dates) == len(rewards):
            date_objects = [datetime.strptime(date, '%Y-%m') for date in dates]
            colors = ['red' if x < 0 else 'blue' for x in rewards]
            plt.bar(date_objects, rewards, color=colors, width=20)
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            plt.gca().xaxis.set_major_locator(mdates.YearLocator())
        else:
            colors = ['red' if x < 0 else 'blue' for x in rewards]
            plt.bar(range(len(rewards)), rewards, color=colors)
        
        plt.title('RL ëª¨ë¸ì˜ ì›”ë³„ ë³´ìƒ')
        plt.xlabel('ì›”')
        plt.ylabel('ë³´ìƒ')
        plt.grid(True, axis='y')
        plt.tight_layout()
        plt.savefig(f'./res/RL/plots/{self.save_name}_rewards.png')
        plt.close()
        
        self.logger.info(f"ê²°ê³¼ ì‹œê°í™” ì™„ë£Œ: ./res/RL/plots/{self.save_name}_*.png")


if __name__ == "__main__":
    # * Config
    with open('./RL/SAC_config.yaml', 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)

    parser = argparse.ArgumentParser()
    parser.add_argument('--eval', type=bool, default=True,
                        help='Evaluates a policy a policy every 10 episode (default: True)')
    parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
                        help='discount factor for reward (default: 0.99)')
    parser.add_argument('--tau', type=float, default=0.005, metavar='G',
                        help='target smoothing coefficient(Ï„) (default: 0.005)')
    parser.add_argument('--lr', type=float, default=0.0001, metavar='G',
                        help='learning rate (default: 0.0001)')
    # parser.add_argument('--alpha', type=float, default=0.2, metavar='G',
    #                     help='Temperature parameter Î± determines the relative importance of the entropy\
    #                             term against the reward (default: 0.2)')
    parser.add_argument('--seed', type=int, default=42, metavar='N',
                        help='random seed (default: 42)')
    parser.add_argument('--batch_size', type=int, default=1024, metavar='N',
                        help='batch size (default: 256)')
    parser.add_argument('--num_collectors', type=int, default=4, metavar='N',
                        help='number of parallel data collectors (default: 4)')
   

    # * Arguments
    # ê¸°ë³¸ì ìœ¼ë¡œ Yamlì„ ë”°ë¥´ë‚˜, argparseë¡œ ë°›ì€ ì¸ìê°€ ìˆë‹¤ë©´ í•´ë‹¹ ì¸ìë¡œ ë®ì–´ì”Œì›€
    args = parser.parse_args()
    config = override_config(config, args)
    
    batch_size = config['batch_size']
    start_steps = config['start_steps']
    n_steps = config['n_steps']
    env_args = config['env_args']
    agent_args = config['agent_args']

    reward_method =  env_args['hard_reward']
    reward_scale = env_args['reward_scale']
    dynamic_gamma = env_args['dynamic_gamma']
    num_cluster = config['num_cluster']
    hidden = agent_args['hidden_size'][0]
    agent_type = agent_args['agent_type']
    kl_weight = agent_args['kl_weight']
    group_size = agent_args['group_size']
    num_layers = len(agent_args['hidden_size'])
    lr = agent_args['lr']
    
    # Generate a unique name for the run
    base_save_name = (
        f'{agent_type}{num_cluster}_{reward_method}_scale{reward_scale}_'
        f'dim{hidden}_g{dynamic_gamma}_layers{num_layers}_'
        f'gsize{group_size}_lr{lr}'
    )
    
    if config['save_name'] == 'automatic':
        save_name = base_save_name
    else:
        prefix = config['save_name']
        save_name = f'{prefix}_{base_save_name}'
    
    # env_args's cluster_dir fix
    env_args['clusters_dir'] = env_args['clusters_dir'] + f'{num_cluster}'
    seed = config['seed']
    replay_size = config['replay_size']
    update_per_step = config['update_per_step']

    logger = set_logger(seed=seed, run_name=save_name, save_dir='./res/RL')
    logger.info("Configuration is:\n" + json.dumps(config, indent=4))

    seed_everything(seed)
    trainer = Trainer(
                    batch_size=batch_size,
                    start_steps=start_steps,
                    n_steps=n_steps,
                    save_name=save_name,
                    device='cuda',
                    seed=seed,
                    env_args=env_args,
                    agent_args=agent_args,
                    update_per_step= update_per_step,
                    replay_size=replay_size,
                    logger=logger,
                    use_per=config.get('use_per', False),
                    per_alpha=config.get('per_alpha', 0.6),
                    per_beta=config.get('per_beta', 0.4),
                    eval_interval=config.get('eval_interval', 10),  # YAMLì—ì„œ ì½ì–´ì˜¤ê¸°
                    eval_episodes=config.get('eval_episodes', 10), # YAMLì—ì„œ ì½ì–´ì˜¤ê¸°
                    num_collectors=config.get('num_collectors', 4)
                    )
    
    trainer.train()
    total_reward, rewards, actions, portfolio_values, states = trainer.inference('./res/RL/models', save_name)
    # final logging for its name
    # save actions
    action_df = pd.DataFrame(actions)
    action_df.to_csv(f'./res/RL/results/action_details_{save_name}.csv')
    
    logger.info(f"Name of this run: {save_name}")